{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIAF1B61CG-k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Concatenate, Input, ReLU, MultiHeadAttention, Layer\n",
        "import keras\n",
        "\n",
        "from tensorflow.keras.losses import Loss\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras import backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import re\n",
        "from scipy.stats import rankdata, norm\n",
        "from scipy.sparse import load_npz\n",
        "import gc\n",
        "import csv\n",
        "import pickle\n",
        "import glob\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zdEBZZqCSZR",
        "outputId": "6fe49428-d82e-4456-c71b-6d33275b87fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG7d8wmrCT0n"
      },
      "outputs": [],
      "source": [
        "#utility code\n",
        "\n",
        "# Prepares a tsv file for submission from model output\n",
        "class PredictionPreparation:\n",
        "    def __init__(self, test_pred, Y_labels, protein_labels):\n",
        "        self.test_pred = test_pred\n",
        "        self.Y_labels = Y_labels\n",
        "        self.protein_labels = protein_labels\n",
        "\n",
        "    def prepare_submission(self, filename=\"submission.tsv\", top_percent=1, dropweights=False, dropzeros=True):\n",
        "        # prepare dataframe for submission\n",
        "        df_finalSubmission_MF = pd.DataFrame(columns = ['Protein Id', 'GO Term Id','Prediction'])\n",
        "        l = []\n",
        "        for k in list(self.protein_labels):\n",
        "            l += [k] * self.test_pred.shape[1]\n",
        "        df_finalSubmission_MF['Protein Id'] = l\n",
        "        df_finalSubmission_MF['GO Term Id'] = list(self.Y_labels) * self.test_pred.shape[0]\n",
        "        df_finalSubmission_MF['Prediction'] = self.test_pred.ravel()\n",
        "\n",
        "        # Calculate the number of rows that represent the top n%\n",
        "        top_rows = int(top_percent * df_finalSubmission_MF.shape[0])\n",
        "\n",
        "        # Select the top 5% rows based on 'Prediction'\n",
        "        df_finalSubmission_MF = df_finalSubmission_MF.nlargest(top_rows, 'Prediction')\n",
        "\n",
        "        if dropzeros:\n",
        "            # Drop rows where 'Prediction' is equal or less than zero\n",
        "            print('zeros dropped for', len(df_finalSubmission_MF[df_finalSubmission_MF['Prediction'] <= 0.0001]), 'rows')\n",
        "            df_finalSubmission_MF = df_finalSubmission_MF[df_finalSubmission_MF['Prediction'] > 0.0001]\n",
        "\n",
        "\n",
        "        # round to 3 decimals\n",
        "        df_finalSubmission_MF['Prediction'] = df_finalSubmission_MF['Prediction'].round(3)\n",
        "\n",
        "        if dropweights:\n",
        "            # Drop the right most column (weights)\n",
        "            df_finalSubmission_MF = df_finalSubmission_MF.iloc[:, :-1]\n",
        "            print(df_finalSubmission_MF.head())\n",
        "            print('matrix shape:', df_finalSubmission_MF.shape)\n",
        "\n",
        "        else:\n",
        "            # take a quick look at the data\n",
        "            summary_stats = df_finalSubmission_MF['Prediction'].describe()\n",
        "            print(df_finalSubmission_MF.head())\n",
        "            print(summary_stats)\n",
        "\n",
        "        # Save to file\n",
        "        df_finalSubmission_MF.to_csv(filename, header=False, index=False, sep=\"\\t\")\n",
        "\n",
        "# prepares training data depending on which ontology is being looked at\n",
        "class TrainingDataPreparation:\n",
        "    def __init__(self, ontology):\n",
        "        self.ontology = ontology\n",
        "\n",
        "    def load_Y_data(self):\n",
        "        if self.ontology == 'BPO':\n",
        "            Y = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Y_BP_1500.npy', allow_pickle=True)\n",
        "            Y_labels = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Y_BP_labels_1500.npy', allow_pickle=True)\n",
        "            df = pd.read_csv('/content/drive/MyDrive/CAFA5 2023/Train_data/BPO_1500_freq_weights.csv')\n",
        "            weights_raw = df['IA_weight'].values.tolist()\n",
        "            weights = {i: weights_raw[i] for i in range(len(weights_raw))}\n",
        "\n",
        "        elif self.ontology == 'CCO':\n",
        "            Y = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Y_CC_800.npy', allow_pickle=True)\n",
        "            Y_labels = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Y_CC_labels_800.npy', allow_pickle=True)\n",
        "            df = pd.read_csv('/content/drive/MyDrive/CAFA5 2023/Train_data/CCO_800_freq_weights.csv')\n",
        "            weights_raw = df['IA_weight'].values.tolist()\n",
        "            weights = {i: weights_raw[i] for i in range(len(weights_raw))}\n",
        "\n",
        "        elif self.ontology == 'MFO':\n",
        "            Y = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Y_MF_800.npy', allow_pickle=True)\n",
        "            Y_labels = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Y_MF_labels_800.npy', allow_pickle=True)\n",
        "            df = pd.read_csv('/content/drive/MyDrive/CAFA5 2023/Train_data/MFO_800_freq_weights.csv')\n",
        "            weights_raw = df['IA_weight'].values.tolist()\n",
        "            weights = {i: weights_raw[i] for i in range(len(weights_raw))}\n",
        "\n",
        "        else:\n",
        "            print('Error: ontology not recognized')\n",
        "            return None, None, None\n",
        "        print(f'Loaded {self.ontology} ontology')\n",
        "\n",
        "        return Y, Y_labels, weights, df\n",
        "\n",
        "    def load_t5_data(self):\n",
        "        train_data1 = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/t5_data_sorted_f32.npy', mmap_mode='r')\n",
        "        test_data1 = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/t5_data_sorted.npy', mmap_mode='r')\n",
        "\n",
        "        print(f'T5 train data shape: {train_data1.shape}')\n",
        "        print(f'T5 test data shape: {test_data1.shape}')\n",
        "\n",
        "        return train_data1, test_data1\n",
        "\n",
        "    def load_esm2_s_data(self):\n",
        "        train_data2 = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/ems_data_sorted.npy', mmap_mode='r')\n",
        "        test_data2 = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/ems_data_sorted.npy', mmap_mode='r')\n",
        "\n",
        "        print(f'ESM2 small train data shape: {train_data2.shape}')\n",
        "        print(f'ESM2 small test data shape: {test_data2.shape}')\n",
        "\n",
        "        return train_data2, test_data2\n",
        "\n",
        "    def load_esm2_l_data(self):\n",
        "        train_data3 = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/ESM2_3B_train_embeddings_sorted.npy', mmap_mode='r')\n",
        "        test_data3 = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/ESM2_3B_test_embeddings_sorted.npy', mmap_mode='r')\n",
        "\n",
        "        print(f'ESM2 3B train data shape: {train_data3.shape}')\n",
        "        print(f'ESM2 3B data shape: {test_data3.shape}')\n",
        "\n",
        "        return train_data3, test_data3\n",
        "\n",
        "    def load_pb_data(self):\n",
        "        train_data4 = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/pb_data_sorted_f32.npy', mmap_mode='r')\n",
        "        test_data4 = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/pb_data_sorted.npy', mmap_mode='r')\n",
        "\n",
        "        print(f'PB train data shape: {train_data4.shape}')\n",
        "        print(f'PB data shape: {test_data4.shape}')\n",
        "\n",
        "        return train_data4, test_data4\n",
        "\n",
        "    def load_ankh_data(self):\n",
        "        train_data5 = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Ankh_train_embeddings_sorted.npy', mmap_mode='r')\n",
        "        test_data5 = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/Ankh_test_embeddings_sorted.npy', mmap_mode='r')\n",
        "\n",
        "        print(f'Ankh train data shape: {train_data5.shape}')\n",
        "        print(f'Ankh data shape: {test_data5.shape}')\n",
        "\n",
        "        return train_data5, test_data5\n",
        "\n",
        "    def load_taxa_data(self):\n",
        "        train_data6 = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/protein_taxa_matrix_train.npy', mmap_mode='r')\n",
        "        test_data6 = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/protein_taxa_matrix_test.npy', mmap_mode='r')\n",
        "\n",
        "        print(f'Taxa train data shape: {train_data6.shape}')\n",
        "        print(f'Taxa data shape: {test_data6.shape}')\n",
        "\n",
        "        return train_data6, test_data6\n",
        "\n",
        "    def load_text_embed(self):\n",
        "\n",
        "        train_data7 = np.load('/content/drive/MyDrive/CAFA5 2023/Absrtract_embeds_modified/abstract_embeds_train_sorted_all.npy', mmap_mode='r')\n",
        "        test_data7 = np.load('/content/drive/MyDrive/CAFA5 2023/Absrtract_embeds_modified/abstract_embeds_test_sorted_all.npy', mmap_mode='r')\n",
        "\n",
        "        print(f'Text abstract data shape: {train_data7.shape}')\n",
        "        print(f'Text abstract shape: {test_data7.shape}')\n",
        "\n",
        "        return train_data7, test_data7\n",
        "\n",
        "class PredictionPreparation:\n",
        "    def __init__(self, test_pred, Y_labels, protein_labels):\n",
        "        self.test_pred = test_pred\n",
        "        self.Y_labels = Y_labels\n",
        "        self.protein_labels = protein_labels\n",
        "\n",
        "    def prepare_submission(self, filename=\"submission.tsv\", top_percent=1, dropweights=False, dropzeros=True):\n",
        "        # prepare dataframe for submission\n",
        "        df_finalSubmission_MF = pd.DataFrame(columns = ['Protein Id', 'GO Term Id','Prediction'])\n",
        "        l = []\n",
        "        for k in list(self.protein_labels):\n",
        "            l += [k] * self.test_pred.shape[1]\n",
        "        df_finalSubmission_MF['Protein Id'] = l\n",
        "        df_finalSubmission_MF['GO Term Id'] = list(self.Y_labels) * self.test_pred.shape[0]\n",
        "        df_finalSubmission_MF['Prediction'] = self.test_pred.ravel()\n",
        "\n",
        "        # Calculate the number of rows that represent the top n%\n",
        "        top_rows = int(top_percent * df_finalSubmission_MF.shape[0])\n",
        "\n",
        "        # Select the top 5% rows based on 'Prediction'\n",
        "        df_finalSubmission_MF = df_finalSubmission_MF.nlargest(top_rows, 'Prediction')\n",
        "\n",
        "        if dropzeros:\n",
        "            # Drop rows where 'Prediction' is equal or less than zero\n",
        "            print('zeros dropped for', len(df_finalSubmission_MF[df_finalSubmission_MF['Prediction'] <= 0.0001]), 'rows')\n",
        "            df_finalSubmission_MF = df_finalSubmission_MF[df_finalSubmission_MF['Prediction'] > 0.0001]\n",
        "\n",
        "\n",
        "        # round to 3 decimals\n",
        "        df_finalSubmission_MF['Prediction'] = df_finalSubmission_MF['Prediction'].round(3)\n",
        "\n",
        "        if dropweights:\n",
        "            # Drop the right most column (weights)\n",
        "            df_finalSubmission_MF = df_finalSubmission_MF.iloc[:, :-1]\n",
        "            print(df_finalSubmission_MF.head())\n",
        "            print('matrix shape:', df_finalSubmission_MF.shape)\n",
        "\n",
        "        else:\n",
        "            # take a quick look at the data\n",
        "            summary_stats = df_finalSubmission_MF['Prediction'].describe()\n",
        "            print(df_finalSubmission_MF.head())\n",
        "            print(summary_stats)\n",
        "\n",
        "        # Save to file\n",
        "        df_finalSubmission_MF.to_csv(filename, header=False, index=False, sep=\"\\t\")\n",
        "\n",
        "class ProteinPredictions:\n",
        "    # Initialize an empty dictionary to store the predictions\n",
        "    def __init__(self):\n",
        "        self.predictions = {}\n",
        "\n",
        "    # Add a prediction to the storage, with optional bonus\n",
        "    # Arguments:\n",
        "    #   - protein: Identifier for the protein\n",
        "    #   - go_term: GO term that is being predicted\n",
        "    #   - score: Confidence score of the prediction\n",
        "    #   - branch: Branch of the Gene Ontology (e.g., 'CCO', 'MFO', 'BPO')\n",
        "    #   - bonus: Optional bonus to be added to the score\n",
        "    def add_prediction(self, protein, go_term, score, branch, bonus=1, adjustment=1):\n",
        "        # If the protein is not already in the storage, initialize its structure\n",
        "        if protein not in self.predictions:\n",
        "            self.predictions[protein] = {'CCO': {}, 'MFO': {}, 'BPO': {}}\n",
        "\n",
        "        # Convert the score to a float for comparison and calculation\n",
        "        score = float(score)\n",
        "\n",
        "        # If this GO term has already been predicted for this protein and branch,\n",
        "        # add the bonus to the score. Keep the highest score.\n",
        "        if go_term in self.predictions[protein][branch]:\n",
        "            self.predictions[protein][branch][go_term] *= 1+(score**3)*bonus\n",
        "            self.predictions[protein][branch][go_term] += score*adjustment\n",
        "\n",
        "        # If this GO term has not been predicted yet, store it with the score\n",
        "        else:\n",
        "            self.predictions[protein][branch][go_term] = max(score*adjustment, 0)\n",
        "\n",
        "        # Ensure that the score does not exceed 1\n",
        "        if self.predictions[protein][branch][go_term] > 1:\n",
        "            self.predictions[protein][branch][go_term] = 1\n",
        "\n",
        "    # Get a list of all scores in the predictions\n",
        "    def get_scores(self):\n",
        "        scores = []\n",
        "        for protein, branches in self.predictions.items():\n",
        "            for branch, go_terms in branches.items():\n",
        "                scores.extend(go_terms.values())\n",
        "        return scores\n",
        "\n",
        "    def plot_predictions(self):\n",
        "        scores = self.get_scores()\n",
        "        plt.hist(scores, bins=30, edgecolor='black')\n",
        "        plt.title('Distribution of Prediction Scores')\n",
        "        plt.xlabel('Score')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "\n",
        "    # Export the stored predictions to a file\n",
        "    # Arguments:\n",
        "    #   - output_file: File name for the exported predictions\n",
        "    #   - top: Number of top predictions to export for each protein and branch\n",
        "    def get_predictions(self, output_file='submission.tsv', top=60):\n",
        "        # Open the output file\n",
        "        with open(output_file, 'w') as f:\n",
        "            # Iterate through each protein and its branches\n",
        "            for protein, branches in self.predictions.items():\n",
        "                # For each branch, sort the GO terms by score in descending order and select the top ones\n",
        "                for branch, go_terms in branches.items():\n",
        "                    # Sort go_terms by score in descending order and take the top ones\n",
        "                    top_go_terms = sorted(go_terms.items(), key=lambda x: x[1], reverse=True)[:top]\n",
        "                    # Write each of the top predictions to the file\n",
        "                    for go_term, score in top_go_terms:\n",
        "                        f.write(f\"{protein}\\t{go_term}\\t{score:.3f}\\n\")\n",
        "\n",
        "\n",
        "def extract_go_terms_and_branches(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "        # Match each stanza with [Term] in the OBO file\n",
        "        stanzas = re.findall(r'\\[Term\\][\\s\\S]*?(?=\\n\\[|$)', content)\n",
        "\n",
        "    go_terms_dict = {}\n",
        "    for stanza in stanzas:\n",
        "        # Extract the GO term ID\n",
        "        go_id = re.search(r'^id: (GO:\\d+)', stanza, re.MULTILINE)\n",
        "        if go_id:\n",
        "            go_id = go_id.group(1)\n",
        "\n",
        "        # Extract the namespace (branch)\n",
        "        namespace = re.search(r'^namespace: (\\w+)', stanza, re.MULTILINE)\n",
        "        if namespace:\n",
        "            namespace = namespace.group(1)\n",
        "\n",
        "        if go_id and namespace:\n",
        "            # Map the branch abbreviation to the corresponding BPO, CCO, or MFO\n",
        "            branch_abbr = {'biological_process': 'BPO', 'cellular_component': 'CCO', 'molecular_function': 'MFO'}\n",
        "            go_terms_dict[go_id] = branch_abbr[namespace]\n",
        "\n",
        "    return go_terms_dict\n",
        "\n",
        "def get_constr_out(x, R):\n",
        "    \"\"\" Given the output of the neural network x returns the output of MCM given the hierarchy constraint expressed in the matrix R \"\"\"\n",
        "    c_out = tf.cast(x, tf.float32)\n",
        "    print('c_out1', c_out.shape)\n",
        "    c_out = tf.expand_dims(c_out, 1)\n",
        "    print('c_out2', c_out.shape)\n",
        "    c_out = tf.tile(c_out, [1, tf.shape(R)[1], 1])\n",
        "    print('c_out3', c_out.shape)\n",
        "    # c_out = tf.expand_dims(c_out, -2)  # Expand the last dimension of c_out\n",
        "    # print('c_out2', c_out.shape)\n",
        "    # c_out = tf.tile(c_out, [1, tf.shape(R)[1]], 1)  # Make c_out match the shape of R\n",
        "    # print('c_out3', c_out.shape)\n",
        "    R = tf.cast(R, tf.float32)\n",
        "    print('R', R.shape)\n",
        "    R = tf.expand_dims(R, axis=0)  # make R 3D by adding an extra dimension\n",
        "    print('R', R.shape)\n",
        "    R_batch = tf.tile(R, [tf.shape(x)[0], 1, 1])  # replicate R along the batch dimension\n",
        "    print('R_batch', R_batch.shape)\n",
        "    print(R_batch[0][50][0:50])\n",
        "    print(c_out[0][0][:50])\n",
        "    final_out = tf.reduce_max(R_batch * c_out, axis=2)\n",
        "    print('final_out', final_out.shape)\n",
        "    return final_out\n",
        "\n",
        "class CustomMCM(Loss):\n",
        "    def __init__(self, R, name=\"custom_mcm\"):\n",
        "        super().__init__(name=name)\n",
        "        self.R = tf.cast(R, tf.float32)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        constr_output = get_constr_out(y_pred, self.R)\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        print('yt, yp, c_out', y_true.shape, y_pred.shape, constr_output.shape)\n",
        "        train_output = y_true*y_pred\n",
        "        print('train_output1', train_output.shape)\n",
        "        train_output = get_constr_out(train_output, self.R)\n",
        "        print('train_output2', train_output.shape)\n",
        "        train_output = (1-y_true)*constr_output + y_true*train_output\n",
        "        print('train_output3', train_output.shape)\n",
        "        # return binary cross entropy loss\n",
        "        #return K.mean(K.binary_crossentropy(y_true, train_output), axis=-1) #this does not make things better -1 -> -2\n",
        "        return tf.keras.losses.binary_crossentropy(y_true, train_output)\n",
        "\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CAFA5 2023/Train_data/go-basic.obo'\n",
        "go_terms_dict = extract_go_terms_and_branches(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06mgscVACVwS",
        "outputId": "4384440e-70dc-4510-f0d4-07d046e5c0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded CCO ontology\n",
            "T5 train data shape: (142246, 1024)\n",
            "T5 test data shape: (141864, 1024)\n",
            "ESM2 small train data shape: (142246, 1280)\n",
            "ESM2 small test data shape: (141864, 1280)\n",
            "ESM2 3B train data shape: (142246, 2560)\n",
            "ESM2 3B data shape: (141864, 2560)\n",
            "PB train data shape: (142246, 1024)\n",
            "PB data shape: (141864, 1024)\n",
            "Ankh train data shape: (142246, 1536)\n",
            "Ankh data shape: (141864, 1536)\n",
            "Taxa train data shape: (142246, 70)\n",
            "Taxa data shape: (141864, 70)\n",
            "Text abstract data shape: (142246, 10279)\n",
            "Text abstract shape: (141864, 10279)\n",
            "R shape is (800, 800)\n",
            "datasets loaded\n"
          ]
        }
      ],
      "source": [
        "ontology = 'CCO'\n",
        "data_prep = TrainingDataPreparation(ontology)\n",
        "Y, Y_labels, weights, df = data_prep.load_Y_data()\n",
        "t5_train, t5_test = data_prep.load_t5_data()\n",
        "esm1_train, esm1_test = data_prep.load_esm2_s_data()\n",
        "esm2_train, esm2_test = data_prep.load_esm2_l_data()\n",
        "pb_train, pb_test = data_prep.load_pb_data()\n",
        "ankh_train, ankh_test = data_prep.load_ankh_data()\n",
        "taxa_train, taxa_test = data_prep.load_taxa_data()\n",
        "txt1_train, txt1_test = data_prep.load_text_embed()\n",
        "\n",
        "if ontology == 'CCO':\n",
        "    R = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/CCO_800_DAG_matrix.npy')\n",
        "    output_split = [4, 11, 24, 59, 199]\n",
        "\n",
        "elif ontology == 'MFO':\n",
        "    R = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/MFO_800_DAG_matrix.npy')\n",
        "    output_split = [4,14,28,69,176]\n",
        "\n",
        "elif ontology == 'BPO':\n",
        "    R = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/BPO_1500_DAG_matrix.npy')\n",
        "    output_split = [13,45,102,213,465]\n",
        "\n",
        "\n",
        "print (f'R shape is {R.shape}')\n",
        "print('datasets loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW3S-wesCZfN",
        "outputId": "7d38e622-b7c7-4616-a649-e48e8db5d607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of proteins with no CCO terms = 48882 out of 142246\n",
            "we retained 93364 proteins from training set where CCO terms are present\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of proteins with no {ontology} terms = {np.sum(np.all(Y == 0, axis=1))} out of {Y.shape[0]}')\n",
        "\n",
        "valid_rows = ~np.all(Y == 0, axis=1)   #~ inverts np all\n",
        "# Filter out rows in Y\n",
        "Y = Y[valid_rows]\n",
        "\n",
        "# Filter out rows in other input arrays\n",
        "t5_train = t5_train[valid_rows]\n",
        "esm1_train = esm1_train[valid_rows]\n",
        "esm2_train = esm2_train[valid_rows]\n",
        "pb_train = pb_train[valid_rows]\n",
        "ankh_train = ankh_train[valid_rows]\n",
        "taxa_train = taxa_train[valid_rows]\n",
        "txt1_train = txt1_train[valid_rows]\n",
        "\n",
        "print(f'we retained {np.sum(valid_rows)} proteins from training set where {ontology} terms are present')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bEO0SWAPCbN3"
      },
      "outputs": [],
      "source": [
        "# Setup vars\n",
        "n_epochs = 20\n",
        "n_epochs_ft = 10\n",
        "dropout_rate = 0.5\n",
        "l1_dim = 600\n",
        "l2_dim = 300\n",
        "final_dim = 800\n",
        "\n",
        "txt_dim1 = 200\n",
        "txt_dim2 = 200\n",
        "\n",
        "#loss without removing any training data (proteins with no BPO...) = 0.0310\n",
        "\n",
        "#txt dim tests with BPO loss numbers\n",
        "#no text input = 0.04613\n",
        "#600/300 = 0.04313\n",
        "#400/200 = 0.04302\n",
        "#300/150 = 0.04295\n",
        "#200/200 = 0.04280\n",
        "  #final_dim 800 -> 400 = 0.04343\n",
        "  #final dense layer removed completely -> 0.04286\n",
        "#200/100 = 0.04302\n",
        "#140/140 = 0.04306\n",
        "\n",
        "#modifying the l1_dim 600 -> 300 = 0.04297\n",
        "#l2_dim 300->450 = 0.04281\n",
        "#longer training txt dim 100/100 = 0.04309"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wiaWzCbUCdfJ"
      },
      "outputs": [],
      "source": [
        "rand_state_no = 1\n",
        "#output_split = [10,20,30,40,50]\n",
        "output_split = [5,10,20,40,80]\n",
        "\n",
        "# Define 5-fold cross validation\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=rand_state_no)\n",
        "\n",
        "fold_ct = 1\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in kf.split(t5_train):\n",
        "\n",
        "    # Skip other folds for testing\n",
        "    # if fold_ct>1:\n",
        "    #   continue\n",
        "\n",
        "    # Define save name\n",
        "    save_name = '/content/drive/MyDrive/CAFA5 2023/1.30/'+f\"{ontology}_fc{fold_ct}_r{rand_state_no}_1.30.5\"\n",
        "\n",
        "    x_tr_1, x_val_1 = t5_train[train_index], t5_train[val_index]\n",
        "    x_tr_2, x_val_2 = esm1_train[train_index], esm1_train[val_index]\n",
        "    x_tr_3, x_val_3 = esm2_train[train_index], esm2_train[val_index]\n",
        "    x_tr_4, x_val_4 = pb_train[train_index], pb_train[val_index]\n",
        "    x_tr_5, x_val_5 = ankh_train[train_index], ankh_train[val_index]\n",
        "    x_tr_6, x_val_6 = taxa_train[train_index], taxa_train[val_index]\n",
        "    x_tr_7, x_val_7 = txt1_train[train_index], txt1_train[val_index]\n",
        "    # x_tr_8, x_val_8 = txt2_train[train_index], txt2_train[val_index]\n",
        "    # x_tr_9, x_val_9 = txt3_train[train_index], txt3_train[val_index]\n",
        "\n",
        "    y_tr_1, y_val_1 = Y[train_index,:output_split[0]], Y[val_index,:output_split[0]]\n",
        "    y_tr_2, y_val_2 = Y[train_index,output_split[0]:output_split[1]], Y[val_index,output_split[0]:output_split[1]]\n",
        "    y_tr_3, y_val_3 = Y[train_index,output_split[1]:output_split[2]], Y[val_index,output_split[1]:output_split[2]]\n",
        "    y_tr_4, y_val_4 = Y[train_index,output_split[2]:output_split[3]], Y[val_index,output_split[2]:output_split[3]]\n",
        "    y_tr_5, y_val_5 = Y[train_index,output_split[3]:output_split[4]], Y[val_index,output_split[3]:output_split[4]]\n",
        "    y_tr_6, y_val_6 = Y[train_index,output_split[4]:], Y[val_index,output_split[4]:]\n",
        "\n",
        "    # Define the inputs\n",
        "    t5_in = Input(shape=(t5_train.shape[1],))\n",
        "    esm1_in = Input(shape=(esm1_train.shape[1],))\n",
        "    mlpr_in = Input(shape=(esm2_train.shape[1],))\n",
        "    knn_in = Input(shape=(pb_train.shape[1],))\n",
        "    lr_in = Input(shape=(ankh_train.shape[1],))\n",
        "    taxa_in = Input(shape=(taxa_train.shape[1],))\n",
        "    txt1_in = Input(shape=(txt1_train.shape[1],))\n",
        "    # txt2_in = Input(shape=(txt2_train.shape[1],))\n",
        "    # txt3_in = Input(shape=(txt3_train.shape[1],))\n",
        "\n",
        "    network_1 = Dense(l1_dim, name='t5_dense_1')(t5_in)\n",
        "    network_1 = BatchNormalization(name='t5_batchnorm_1')(network_1)\n",
        "    network_1 = LeakyReLU(alpha=0.1, name='t5_leakyrelu_1')(network_1)\n",
        "    network_1 = Dropout(dropout_rate, name='t5_dropout_1')(network_1)\n",
        "    network_1 = Dense(l2_dim, name='t5_dense_2')(network_1)\n",
        "\n",
        "    network_2 = Dense(l1_dim, name='esm1_dense_1')(esm1_in)\n",
        "    network_2 = BatchNormalization(name='esm1_batchnorm_1')(network_2)\n",
        "    network_2 = LeakyReLU(alpha=0.1, name='esm1_leakyrelu_1')(network_2)\n",
        "    network_2 = Dropout(dropout_rate, name='esm1_dropout_1')(network_2)\n",
        "    network_2 = Dense(l2_dim, name='esm1_dense_2')(network_2)\n",
        "\n",
        "    network_3 = Dense(l1_dim, name='mlpr_dense_1')(mlpr_in)\n",
        "    network_3 = BatchNormalization(name='mlpr_batchnorm_1')(network_3)\n",
        "    network_3 = LeakyReLU(alpha=0.1, name='mlpr_leakyrelu_1')(network_3)\n",
        "    network_3 = Dropout(dropout_rate, name='mlpr_dropout_1')(network_3)\n",
        "    network_3 = Dense(l2_dim, name='mlpr_dense_2')(network_3)\n",
        "\n",
        "    network_4 = Dense(l1_dim, name='knn_dense_1')(knn_in)\n",
        "    network_4 = BatchNormalization(name='knn_batchnorm_1')(network_4)\n",
        "    network_4 = LeakyReLU(alpha=0.1, name='knn_leakyrelu_1')(network_4)\n",
        "    network_4 = Dropout(dropout_rate, name='knn_dropout_1')(network_4)\n",
        "    network_4 = Dense(l2_dim, name='knn_dense_2')(network_4)\n",
        "\n",
        "    network_5 = Dense(l1_dim, name='lr_dense_1')(lr_in)\n",
        "    network_5 = BatchNormalization(name='lr_batchnorm_1')(network_5)\n",
        "    network_5 = LeakyReLU(alpha=0.1, name='lr_leakyrelu_1')(network_5)\n",
        "    network_5 = Dropout(dropout_rate, name='lr_dropout_1')(network_5)\n",
        "    network_5 = Dense(l2_dim, name='lr_dense_2')(network_5)\n",
        "\n",
        "    network_6 = Dense(128, name='taxa_dense_1')(taxa_in)\n",
        "    network_6 = BatchNormalization(name='taxa_batchnorm_1')(network_6)\n",
        "    network_6 = LeakyReLU(alpha=0.1, name='taxa_leakyrelu_1')(network_6)\n",
        "    network_6 = Dropout(dropout_rate, name='taxa_dropout_1')(network_6)\n",
        "    network_6 = Dense(64, name='taxa_dense_2')(network_6)\n",
        "\n",
        "    network_7 = Dense(txt_dim1, name='txt1_dense_1')(txt1_in)\n",
        "    network_7 = BatchNormalization(name='txt1_batchnorm_1')(network_7)\n",
        "    network_7 = LeakyReLU(alpha=0.1, name='txt1_leakyrelu_1')(network_7)\n",
        "    network_7 = Dropout(dropout_rate, name='txt1_dropout_1')(network_7)\n",
        "    network_7 = Dense(txt_dim2, name='txt1_dense_2')(network_7)\n",
        "\n",
        "    # Concatenate the networks\n",
        "    # combined = Concatenate()([network_1, network_2, network_3, network_4, network_5, network_6, network_7])\n",
        "    # combined = BatchNormalization(name = 'combined_batchnorm_1')(combined)\n",
        "    combined = Dense(final_dim, name='combined_dense_1', activation='relu')(combined)\n",
        "\n",
        "    # First level output\n",
        "    output_1 = Dense(output_split[0], activation='sigmoid', name='final_output_1_112')(combined)\n",
        "\n",
        "    # Second level output, connected to first level\n",
        "    combined_2 = Concatenate()([combined, output_1])\n",
        "    output_2 = Dense(output_split[1]-output_split[0], activation='sigmoid', name='final_output_2_112')(combined_2)\n",
        "\n",
        "    # Third level output, connected to second level\n",
        "    combined_3 = Concatenate()([combined, output_1, output_2])\n",
        "    output_3 = Dense(output_split[2]-output_split[1], activation='sigmoid', name='final_output_3_112')(combined_3)\n",
        "\n",
        "    # Fourth level output, connected to third level\n",
        "    combined_4 = Concatenate()([combined, output_1, output_2, output_3])\n",
        "    output_4 = Dense(output_split[3]-output_split[2], activation='sigmoid', name='final_output_4_112')(combined_4)\n",
        "\n",
        "    # Fifth level output, connected to fourth level\n",
        "    combined_5 = Concatenate()([combined, output_1, output_2, output_3, output_4])\n",
        "    output_5 = Dense(output_split[4]-output_split[3], activation='sigmoid', name='final_output_5_112')(combined_5)\n",
        "\n",
        "    combined_6 = Concatenate()([combined, output_1, output_2, output_3, output_4, output_5])\n",
        "    output_6 = Dense(Y.shape[1]-output_split[4], activation='sigmoid', name='final_output_6_112')(combined_6)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[t5_in, esm1_in, mlpr_in, knn_in, lr_in, taxa_in, txt1_in],\n",
        "                  outputs=[output_1, output_2, output_3, output_4, output_5, output_6])\n",
        "\n",
        "    #lr scheduling\n",
        "    def lr_schedule(epoch, lr):\n",
        "        if epoch > 0 and epoch % 7 == 0:\n",
        "            lr = lr * 0.5\n",
        "        return lr\n",
        "\n",
        "    # Define callbacks\n",
        "    checkpoint_loss = ModelCheckpoint(f'{save_name}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min')\n",
        "    csv_logger = CSVLogger(f'{save_name}.csv')\n",
        "    lr_callback = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "    weights_list = df['IA_weight'].values.tolist()\n",
        "\n",
        "    #Add weights from IA\n",
        "    weights_1 = weights_list[:output_split[0]]\n",
        "    weights_2 = weights_list[output_split[0]:output_split[1]]\n",
        "    weights_3 = weights_list[output_split[1]:output_split[2]]\n",
        "    weights_4 = weights_list[output_split[2]:output_split[3]]\n",
        "    weights_5 = weights_list[output_split[3]:output_split[4]]\n",
        "    weights_6 = weights_list[output_split[4]:]\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0003),\n",
        "    loss={'final_output_1_112': 'binary_crossentropy',\n",
        "          'final_output_2_112': 'binary_crossentropy',\n",
        "          'final_output_3_112': 'binary_crossentropy',\n",
        "          'final_output_4_112': 'binary_crossentropy',\n",
        "          'final_output_5_112': 'binary_crossentropy',\n",
        "          'final_output_6_112': 'binary_crossentropy'},\n",
        "    loss_weights={'final_output_1_112': weights_1,\n",
        "          'final_output_2_112': weights_2,\n",
        "          'final_output_3_112': weights_3,\n",
        "          'final_output_4_112': weights_4,\n",
        "          'final_output_5_112': weights_5,\n",
        "          'final_output_6_112': weights_6},\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit([x_tr_1, x_tr_2, x_tr_3, x_tr_4, x_tr_5, x_tr_6, x_tr_7], [y_tr_1, y_tr_2, y_tr_3, y_tr_4, y_tr_5, y_tr_6],\n",
        "              validation_data=([x_val_1, x_val_2, x_val_3, x_val_4, x_val_5, x_val_6, x_val_7], [y_val_1, y_val_2, y_val_3, y_val_4, y_val_5, y_val_6]),\n",
        "              epochs=n_epochs, batch_size=256,\n",
        "              callbacks=[lr_callback, checkpoint_loss, csv_logger])\n",
        "\n",
        "    # Code helth\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    fold_ct +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_-bXM9yaebr"
      },
      "outputs": [],
      "source": [
        "# # For submission for CAFA test set\n",
        "# test_labels = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/t5_labels_sorted.npy')\n",
        "# rand_state_nos = [1,2,3,4,5]\n",
        "\n",
        "# predictions = []\n",
        "\n",
        "# for rand_state_no in rand_state_nos:\n",
        "\n",
        "#     for fold_ct in [1,2,3,4,5]:\n",
        "\n",
        "#         model_path = f'/content/drive/MyDrive/CAFA5 2023/1.30/{ontology}_fc{str(fold_ct)}_r{rand_state_no}_1.30.1.hdf5'\n",
        "#         #model.load_weights(model_path) #only for 1.24.2\n",
        "#         model = load_model(model_path)\n",
        "\n",
        "#         # Add OOF predictions for this split\n",
        "#         predicted_outputs = model.predict([t5_test, esm1_test, esm2_test, pb_test, ankh_test, taxa_test, txt1_test])\n",
        "#         #output_1, output_2, output_3, output_4, output_5, output_6 = model.predict([x_val_1, x_val_2, x_val_3, x_val_4, x_val_5, x_val_6]) #only for 1.24.3\n",
        "#         #predicted_outputs = np.concatenate([output_1, output_2, output_3, output_4, output_5, output_6], axis=1) #only for 1.24.3\n",
        "\n",
        "#         predictions.append(predicted_outputs)\n",
        "\n",
        "#         K.clear_session()\n",
        "#         gc.collect()\n",
        "\n",
        "# average_predictions = np.mean(predictions, axis=0)\n",
        "# print('average predictions shape:', average_predictions.shape)\n",
        "\n",
        "# prediction_preparation = PredictionPreparation(average_predictions, Y_labels, test_labels)\n",
        "# prediction_preparation.prepare_submission(filename=f\"/content/drive/MyDrive/CAFA5 2023/1.30/{ontology}_test_submission_1.30.1.tsv\", top_percent=0.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For submission for CAFA test set 130b\n",
        "test_labels = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/t5_labels_sorted.npy')\n",
        "model_nos = [2,3]\n",
        "rand_state_no = 1\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for model_no in model_nos:\n",
        "\n",
        "    for fold_ct in [1,2,3,4,5]:\n",
        "\n",
        "        model_path = f'/content/drive/MyDrive/CAFA5 2023/1.30/{ontology}_fc{str(fold_ct)}_r{rand_state_no}_1.30.{model_no}.hdf5'\n",
        "        #model.load_weights(model_path) #only for 1.24.2\n",
        "        model = load_model(model_path)\n",
        "\n",
        "        # Add OOF predictions for this split\n",
        "        #predicted_outputs = model.predict([t5_test, esm1_test, esm2_test, pb_test, ankh_test, taxa_test, txt1_test])\n",
        "        output_1, output_2, output_3, output_4, output_5, output_6 = model.predict([t5_test, esm1_test, esm2_test, pb_test, ankh_test, taxa_test, txt1_test]) #only for 1.24.3\n",
        "        predicted_outputs = np.concatenate([output_1, output_2, output_3, output_4, output_5, output_6], axis=1) #only for 1.24.3\n",
        "\n",
        "        predictions.append(predicted_outputs)\n",
        "\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "average_predictions = np.mean(predictions, axis=0)\n",
        "print('average predictions shape:', average_predictions.shape)\n",
        "\n",
        "prediction_preparation = PredictionPreparation(average_predictions, Y_labels, test_labels)\n",
        "prediction_preparation.prepare_submission(filename=f\"/content/drive/MyDrive/CAFA5 2023/1.30/{ontology}_test_submission_1.30b.tsv\", top_percent=0.05)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrb9d-u980SO",
        "outputId": "68517484-2ac4-4511-91b7-03d49681b5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4434/4434 [==============================] - 16s 4ms/step\n",
            "4434/4434 [==============================] - 16s 3ms/step\n",
            "4434/4434 [==============================] - 15s 3ms/step\n",
            "4434/4434 [==============================] - 15s 3ms/step\n",
            "4434/4434 [==============================] - 16s 4ms/step\n",
            "4434/4434 [==============================] - 16s 3ms/step\n",
            "4434/4434 [==============================] - 16s 3ms/step\n",
            "4434/4434 [==============================] - 16s 3ms/step\n",
            "4434/4434 [==============================] - 16s 3ms/step\n",
            "4434/4434 [==============================] - 17s 4ms/step\n",
            "average predictions shape: (141864, 800)\n",
            "zeros dropped for 0 rows\n",
            "         Protein Id  GO Term Id  Prediction\n",
            "22387210     P12687  GO:0005622         1.0\n",
            "27205610     P31334  GO:0005622         1.0\n",
            "27528010     P32387  GO:0005622         1.0\n",
            "29128810     P36526  GO:0005622         1.0\n",
            "29133610     P36534  GO:0005622         1.0\n",
            "count    5.674560e+06\n",
            "mean     2.107124e-01\n",
            "std      3.012368e-01\n",
            "min      1.600000e-02\n",
            "25%      2.600000e-02\n",
            "50%      5.500000e-02\n",
            "75%      2.340000e-01\n",
            "max      1.000000e+00\n",
            "Name: Prediction, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make groundtruth file for evaluating against\n",
        "np.random.seed(42)  # for reproducibility\n",
        "\n",
        "train_labels = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Ankh_train_labels_sorted.npy')\n",
        "\n",
        "# Set the number of proteins to use as testing for ground truth\n",
        "test_size = 1002\n",
        "\n",
        "# Create groundtruth data for testing\n",
        "random_indices = np.random.choice(t5_train.shape[0], size=test_size, replace=False)\n",
        "random_labels = train_labels[random_indices]\n",
        "ramdom_Y = Y[random_indices]\n",
        "\n",
        "print(f'Check random indices match: {random_indices[:5]}')\n",
        "\n",
        "prediction_preparation = PredictionPreparation(ramdom_Y, Y_labels, random_labels)\n",
        "prediction_preparation.prepare_submission(filename=f\"/content/drive/MyDrive/CAFA5 2023/1.30/Groundtruth_{test_size}_{ontology}_submission.tsv\", top_percent=1, dropweights=True)\n"
      ],
      "metadata": {
        "id": "HcRvfviLyMET",
        "outputId": "f3ad5115-b3cf-455b-f079-f634aebc9c9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check random indices match: [44792 20212 27851 57765 56992]\n",
            "zeros dropped for 791193 rows\n",
            "       Protein Id  GO Term Id\n",
            "104834     Q57VY9  GO:0048471\n",
            "777657     O22179  GO:0140513\n",
            "577620     Q01253  GO:0005773\n",
            "777627     O22179  GO:0005856\n",
            "777629     O22179  GO:0005694\n",
            "matrix shape: (10407, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Internal testing for 1.30.2/3/4\n",
        "train_labels = np.load('/content/drive/MyDrive/CAFA5 2023/Train_data/Ankh_train_labels_sorted.npy')\n",
        "test_labels = np.load('/content/drive/MyDrive/CAFA5 2023/Test_data/t5_labels_sorted.npy')\n",
        "\n",
        "# change for each set of models to be evaluated\n",
        "eval_no = 3\n",
        "\n",
        "np.random.seed(42)  # for reproducibility\n",
        "test_size = 1002\n",
        "\n",
        "model_nos = [2]\n",
        "rand_state_no = 1\n",
        "\n",
        "# Initialize array to accumulate OOF predictions across evaluated models\n",
        "total_oof_predictions = np.zeros_like(Y, dtype=np.float32)\n",
        "\n",
        "for model_no in model_nos:\n",
        "\n",
        "    # Define 5-fold cross validation\n",
        "    n_splits = 5\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=rand_state_no)\n",
        "\n",
        "    fold_ct = 1\n",
        "    out_of_fold_predictions = np.zeros_like(Y, dtype=np.float32) # Initialize array with same shape as Y, MUST BE FLOAT32 OR IT FAILS\n",
        "\n",
        "    # Iterate over each fold and produce prediction on validation data only\n",
        "    for train_index, val_index in kf.split(t5_train):\n",
        "\n",
        "        x_tr_1, x_val_1 = t5_train[train_index], t5_train[val_index]\n",
        "        x_tr_2, x_val_2 = esm1_train[train_index], esm1_train[val_index]\n",
        "        x_tr_3, x_val_3 = esm2_train[train_index], esm2_train[val_index]\n",
        "        x_tr_4, x_val_4 = pb_train[train_index], pb_train[val_index]\n",
        "        x_tr_5, x_val_5 = ankh_train[train_index], ankh_train[val_index]\n",
        "        x_tr_6, x_val_6 = taxa_train[train_index], taxa_train[val_index]\n",
        "        x_tr_7, x_val_7 = txt1_train[train_index], txt1_train[val_index]\n",
        "\n",
        "\n",
        "        model_path = f'/content/drive/MyDrive/CAFA5 2023/1.30/{ontology}_fc{str(fold_ct)}_r{rand_state_no}_1.30.{model_no}.hdf5'\n",
        "        #model.load_weights(model_path) #only for 1.24.2\n",
        "        model = load_model(model_path)\n",
        "\n",
        "        # Add OOF predictions for this split\n",
        "        #predicted_outputs = model.predict([x_val_1, x_val_2, x_val_3, x_val_4, x_val_5, x_val_6, x_val_7])\n",
        "        output_1, output_2, output_3, output_4, output_5, output_6 = model.predict([x_val_1, x_val_2, x_val_3, x_val_4, x_val_5, x_val_6, x_val_7]) #only for 1.24.3\n",
        "        predicted_outputs = np.concatenate([output_1, output_2, output_3, output_4, output_5, output_6], axis=1) #only for 1.24.3\n",
        "\n",
        "        out_of_fold_predictions[val_index, :] = predicted_outputs\n",
        "\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "        fold_ct+=1\n",
        "\n",
        "    total_oof_predictions += out_of_fold_predictions\n",
        "averaged_oof_predictions = total_oof_predictions / len(model_nos)\n",
        "\n",
        "\n",
        "random_indices = np.random.choice(t5_train.shape[0], size=test_size, replace=False)\n",
        "\n",
        "predicted_output = out_of_fold_predictions[random_indices]\n",
        "random_labels = train_labels[random_indices]\n",
        "ramdom_Y = Y[random_indices]\n",
        "\n",
        "print(f'Check random indices match: {random_indices[:5]}')\n",
        "print(f'Output predictions shape:{predicted_output.shape}')\n",
        "\n",
        "prediction_preparation = PredictionPreparation(predicted_output, Y_labels, random_labels)\n",
        "prediction_preparation.prepare_submission(filename=f\"/content/drive/MyDrive/CAFA5 2023/1.30/{ontology}_testing_1.30b.{eval_no}.tsv\", top_percent=0.05)\n",
        "\n",
        "# Code helth\n",
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "#1.30b.5 = 2,3,4 models ensemble\n",
        "#1.30b.6 = 2,3 ensemble\n",
        "#1.30b.7 = just 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quJB6erU7RJn",
        "outputId": "00c150f2-ac4c-456f-86f7-81f42a2b11ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "584/584 [==============================] - 2s 3ms/step\n",
            "584/584 [==============================] - 2s 4ms/step\n",
            "584/584 [==============================] - 2s 4ms/step\n",
            "584/584 [==============================] - 2s 3ms/step\n",
            "584/584 [==============================] - 2s 3ms/step\n",
            "Check random indices match: [44792 20212 27851 57765 56992]\n",
            "Output predictions shape:(1002, 800)\n",
            "zeros dropped for 0 rows\n",
            "       Protein Id  GO Term Id  Prediction\n",
            "85675      P21802  GO:0110165         1.0\n",
            "192875     O75355  GO:0110165         1.0\n",
            "182410     P0C0T0  GO:0005622         1.0\n",
            "182475     P0C0T0  GO:0110165         1.0\n",
            "784075     Q12062  GO:0110165         1.0\n",
            "count    40080.000000\n",
            "mean         0.263313\n",
            "std          0.332948\n",
            "min          0.020000\n",
            "25%          0.035000\n",
            "50%          0.080000\n",
            "75%          0.377000\n",
            "max          1.000000\n",
            "Name: Prediction, dtype: float64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}